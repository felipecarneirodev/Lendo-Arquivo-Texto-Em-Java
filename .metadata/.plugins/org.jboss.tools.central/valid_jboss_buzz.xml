<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0" uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title type="html">Which is better: A monolithic Kafka cluster vs many?</title><link rel="alternate" href="http://www.ofbizian.com/2022/03/which-is-better-monolithic-kafka.html" /><author><name>Unknown</name></author><id>http://www.ofbizian.com/2022/03/which-is-better-monolithic-kafka.html</id><updated>2022-03-30T10:17:00Z</updated><content type="html">Apache Kafka is designed for performance and large volumes of data. Kafka's append-only log format, sequential I/O access, and zero copying all support high throughput with low latency. Its partition-based data distribution lets it scale horizontally to hundreds of thousands of partitions. Because of these capabilities, it can be tempting to use a single monolithic Kafka cluster for all of your eventing needs. Using one cluster reduces your operational overhead and development complexities to a minimum. But is "a single Kafka cluster to rule them all" the ideal architecture, or is it better to split Kafka clusters? To answer that question, we have to consider the segregation strategies for maximizing performance and optimizing cost while increasing Kafka adoption. We also have to understand the impact of using , on a public cloud, or managing it yourself on-premise (Are you looking to experiment with Kafka? Get started in minutes with a no-cost ). This article explores these questions and more, offering a structured way to decide whether or not to segregate Kafka clusters in your organization. Figure 1 summarizes the questions explored in this article.   Figure 1. A mind map for Apache Kafka cluster segregation strategies shows the concerns that can drive a multiple-cluster setup. BENEFITS OF A MONOLITHIC KAFKA CLUSTER To start, let's explore some of the benefits of using a single, monolithic Kafka cluster. Note that by this I don't mean literally a single Kafka cluster for all environments, but a single production Kafka cluster for the entire organization. The different environments would still typically be fully isolated with their respective Kafka clusters. A single production Kafka cluster is simpler to use and operate and is a no-brainer as a starting point. GLOBAL EVENT HUB Many companies are sold on the idea of having a single "Kafka backbone" and the value they can get from it. The possibility of combining data from different topics from across the company arbitrarily in response to future and yet unknown business needs is a huge motivation. As a result, some organizations end up using Kafka as a centralized enterprise service bus (ESB) where they put all their messages under a single cluster. The chain of streaming applications is deeply interconnected. This approach can work for companies with a small number of applications and development teams, and with no hard departmental data boundaries that are enforced in large corporations by business and regulatory forces. (Note that this singleton Kafka environment expects no organizational boundaries.) The monolithic setup reduces thinking about event boundaries, speeds up development, and works well until an operational or a process limitation kicks in. NO TECHNICAL CONSTRAINTS Certain technical features are available only within a single Kafka cluster. For example, a common pattern used by stream processing applications is to perform read-process-write operations in a sequence without any tolerances for errors that could lead to duplicates or loss of messages. To address that strict requirement, Kafka offers transactions that ensure that each message is consumed from the source topic and published to a target topic in exactly-once processing semantics. That guarantee is possible only when the source and target topics are within the same Kafka cluster. A consumer group, such as a , can process data from a single Kafka cluster only. Therefore, multi-topic subscriptions or load balancing across the consumers in a consumer group are possible only within a single Kafka cluster. In a multi-Kafka setup, enabling such stream processing requires data replication across clusters. Each Kafka cluster has a unique URL, a few authentication mechanisms, Kafka-wide authorization configurations, and other cluster-level settings. With a single cluster, all applications can make the same assumptions, use the same configurations, and send all events to the same location. These are all good technical reasons for sharing a single Kafka cluster whenever possible. LOWER COST OF OWNERSHIP I assume that you use Kafka because you have a huge volume of data, or you want to do low latency asynchronous interactions, or take advantage of both of these with added high availability—not because you have modest data needs and Kafka is a fashionable technology. Offering high-volume, low-latency Kafka processing in a production environment has a significant cost. Even a lightly used Kafka cluster deployed for production purposes requires three to six brokers and three to five ZooKeeper nodes. The components should be spread across multiple availability zones for redundancy. Note: ZooKeeper , but its role will still have to be performed by the cluster. You have to budget for base compute, networking, storage, and operating costs for every Kafka cluster. This cost applies whether you self-manage a Kafka cluster on-premises with something like or consume Kafka as a service. There are attempts at "serverless" Kafka offerings that try to be more creative and hide the cost per cluster in other cost lines, but somebody still has to pay for resources. Generally, running and operating multiple Kafka clusters costs more than a single larger cluster. There are exceptions to this rule, where you achieve local cost optimizations by running a cluster at the point where the data and processing happens or by avoiding replication of large volumes of non-critical data, and so on. BENEFITS OF MULTIPLE KAFKA CLUSTERS Although Kafka can scale beyond the needs of a single team, it is not designed for . Sharing a single Kafka cluster across multiple teams and different use cases requires precise application and cluster configuration, a rigorous governance process, standard naming conventions, and best practices for preventing abuse of the shared resources. Using multiple Kafka clusters is an alternative approach to address these concerns. Let's explore a few of the reasons that you might choose to implement multiple Kafka clusters. OPERATIONAL DECOUPLING Kafka's sweet spot is real-time messaging and distributed data processing. Providing that at scale requires operational excellence. Here are a few manageability concerns that apply to operating Kafka. WORKLOAD CRITICALITY Not all Kafka clusters are equal. A batch processing Kafka cluster that can be populated from source again and again with derived data doesn't have to replicate data into multiple sites for higher availability. An ETL data pipeline can afford more downtime than a real-time messaging infrastructure for frontline applications. Segregating workloads by service availability and data criticality helps you pick the most suitable deployment architecture, optimize infrastructure costs, and direct the right level of operating attention to every workload. MAINTAINABILITY The larger a cluster gets, the longer it can take to upgrade and expand the cluster due to rolling restarts, data replication, and rebalancing. In addition to the length of the change window, the time when the change is performed might also be important. A customer-facing application might have an upgrade window that differs from a customer service application. Using separate Kafka clusters allows faster upgrades and more control over the time and the sequence of rolling out a change. REGULATORY COMPLIANCE Regulations and certifications typically leave no room for compromise. You might have to host a Kafka cluster on a specific cloud provider or region. You might have to allow access only to support personnel from a specific country. All personally identifiable information (PII) data might have to be on a particular cluster with short retention, separate administrative access, and network segmentation. You might want to hold the data encryption keys for specific clusters. The larger your company is, the longer the requirements list gets. TENANT ISOLATION The secret for happy application coexistence on a shared infrastructure relies on having good primitives for access, resource, and logical isolation. Unlike , Kafka has no concept like namespaces for enforcing quotas and access control or avoiding topic naming collisions. Let's explore some of the resulting challenges for isolating tenants. RESOURCE ISOLATION Although Kafka has mechanisms to control resource use, it doesn't prevent a bad tenant from monopolizing the cluster resources. Storage size can be controlled per topic through retention size, but cannot be limited for a group of topics corresponding to an application or tenant. Network utilization can be enforced through quotas, but it is applied at the client connection level. There is no means to prevent an application from creating an unlimited number of topics or partitions until the whole cluster gets to a halt. All of that means you have to enforce these resource control mechanisms while operating at different granularity levels, and enforce additional conventions for the healthy coexistence of multiple teams on a single cluster. An alternative is to assign separate Kafka clusters to each functional area and use cluster-level resource isolation. SECURITY BOUNDARY Kafka's access control with the default authorization mechanism (ACLs) is more flexible than the quota mechanism and can apply to multiple resources at once through pattern matching. But you have to ensure good naming convention hygiene. The structure for topic name prefixes becomes part of your security policy. ACLs control which users can perform which actions on which resources, but a user with admin access to a Kafka instance has access to all the topics in that Kafka instance. With multiple clusters, each team can have admin rights only to their Kafka instance. The alternative is to ask someone with admin rights to edit the ACLs and update topics rights and such. Nobody likes having to open a ticket to another team to get a project rolling. LOGICAL DECOUPLING A single cluster shared across multiple teams and applications with different needs can quickly get cluttered and difficult to navigate. You might have teams that need very few topics and others that generate hundreds of them. Some teams might even generate topics on the fly from existing data sources by . You might need hundreds of granular ACLs for some applications that are less trusted, and coarse-grained ACLs for others. You might have a large number of producers and consumers. In the absence of namespaces, properties, and labels that can be used for logical segregation of resources, the only option left is to use naming conventions creatively. USE CASE OPTIMIZATION So far we have looked at the manageability and multi-tenancy needs that apply to most shared platforms in common. Next, we will look at a few examples of Kafka cluster segregation for specific use cases. The goal of this section is to list the long tail of reasons for segregating Kafka clusters that varies for every organization and demonstrate that there is no "wrong" reason for creating another Kafka cluster. DATA LOCALITY Data has gravity, meaning that a useful dataset tends to attract related services and applications. The larger a dataset is, the harder it is to move around. Data can originate from a constrained or offline environment, preventing it from streaming into the cloud. Large volumes of data might reside in a specific region, making it economically unfeasible to replicate the data to other locations. Therefore, you might create separate Kafka clusters at regions, cloud providers, or even to benefit from data's gravitational characteristics. FINE-TUNING Fine-tuning is the process of precisely adjusting the parameters of a system to fit certain objectives. In the Kafka world, the primary interactions that an application has with a cluster center on the concept of topics. And while every topic has separate and fine-tuning configurations, there are also cluster-wide settings that apply to all applications. For instance, cluster-wide configurations such as redundancy factor (RF) and in-sync replicas (ISR) apply to all topics if not explicitly overridden per topic. In addition, some constraints apply to the whole cluster and all users, such as the allowed authentication and authorization mechanisms, IP whitelists, the maximum message size, whether dynamic topic creation is allowed, and so on. Therefore, you might create separate clusters for large messages, less-secure authentication mechanisms, and other oddities to localize and isolate the effect of such configurations from the rest of the tenants. DOMAIN OWNERSHIP Previous sections described examples of cluster segregation to address data and application concerns, but what about business domains? Aligning Kafka clusters by business domain can enforce ownership and give users more responsibilities. Domain-specific clusters can offer more freedom to the domain owners and reduce reliance on a central team. This division can also reduce cross-cluster data replication needs because most joins are likely to happen within the boundaries of a business domain. PURPOSE-BUILT Kafka clusters can be created and configured for a particular use case. Some clusters might be born while and others created while implementing event-driven . Some clusters might be created to handle unpredictable loads, whereas others might be optimized for stable and predictable processing. For example, for stream processing with topic compaction enabled, separate clusters for service communication with short message retention, and a logging cluster for log aggregation. for producers and consumers. The so-called fronting clusters are responsible for getting messages from all applications and buffering, while consumer clusters contain only a subset of the data needed for stream processing. These decisions for classifying clusters are based on high-level criteria, but you might also have low-level criteria for separate clusters. For example, to benefit from page caching at the operating-system level, you might create a separate cluster for consumers that re-read topics from the beginning each time. The separate cluster would prevent any disruption of the page caches for real-time consumers that read data from the current head of each topic. You might also create a separate cluster for the odd use case of a single topic that uses the whole cluster. The reasons can be endless. SUMMARY The argument "one thing to rule them all" has been used for pretty much any technology: mainframes, databases, application servers, ESBs, Kubernetes, cloud providers, and so on. But generally, the principle falls apart. At some point, decentralizing and scaling with multiple instances offer more benefits than continuing with one centralized instance. Then a new threshold is reached, and the technology cycle starts to centralize again, which sparks the next phase of innovation. Kafka is following this historical pattern. In this article, we looked at common motivations for growing a monolithic Kafka cluster along with reasons for splitting it out. But not all points apply to all organizations in every circumstance. Every organization has different business goals and execution strategies, team structure, application architecture, and data processing needs. Every organization is at a different stage of its , a cloud-based architecture, edge computing, data mesh—you name it. You might run on-premises Kafka clusters for good reason and give more weight to the operational concerns you have to deal with. Software-as-a-Service (SaaS) offerings such as can provision a Kafka cluster with a single click and remove the concerns around maintainability, workload criticality, and compliance. With such services, you might pay more attention to governance, logical isolation, and controlling data locality. If you have a reasonably sized organization, you will have hybrid and multi-cloud Kafka deployments and a new set of concerns around optimizing and reusing Kafka skills, patterns, and best practices across the organization. These concerns are topics for another article. I hope this guide provides a way to structure your decision-making process for segregating Kafka clusters. Follow me at to join my journey of learning Apache Kafka. This post was originally published on Red Hat Developers. To read the original post, check .</content><dc:creator>Unknown</dc:creator></entry><entry><title>Generate and save an HTML report in Jenkins on OpenShift 4</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/03/30/generate-and-save-html-report-jenkins-openshift-4" /><author><name>Muhammad Edwin</name></author><id>ed097a95-68ee-4774-919c-153dddb7b95d</id><updated>2022-03-30T07:00:00Z</updated><published>2022-03-30T07:00:00Z</published><summary type="html">&lt;p&gt; &lt;/p&gt; &lt;p&gt;Jenkins is one of the most popular &lt;a href="https://developers.redhat.com/topics/ci-cd"&gt;CI/CD&lt;/a&gt; tools for automating builds and deployments. It is very flexible and can be deployed on almost every operating system, as well as on &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt;. This article shows you how to deploy Jenkins on OpenShift 4.9, create a simple pipeline to deploy a &lt;a href="https://developers.redhat.com/topics/enterprise-java"&gt;Java&lt;/a&gt; application to OpenShift, do some testing, save the test results as HTML, and publish it as an artifact so that people can see the results.&lt;/p&gt; &lt;p&gt;For this scenario, we'll generate an HTML report using &lt;a href="https://jeremylong.github.io/DependencyCheck/dependency-check-maven/plugin-info.html"&gt;Maven OWASP Dependency Check plugins&lt;/a&gt;. The report will contain a list of libraries that contain vulnerabilities. This pipeline runs on Jenkins 2.2 on top of OpenShift 4.9.&lt;/p&gt; &lt;h2&gt;Use Jenkins to generate a report on OpenShift 4&lt;/h2&gt; &lt;p&gt;There are multiple ways to set up Jenkins on OpenShift 4. This article uses a template provided by the OpenShift Developer Catalog.&lt;/p&gt; &lt;p&gt;First, check whether the Jenkins template is available in OpenShift:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc get template -n openshift | grep jenkins&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If Jenkins templates are available, you'll get output such as:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;jenkins-ephemeral Jenkins service, without persistent storage.... 8 (all set) 7 jenkins-ephemeral-monitored Jenkins service, without persistent storage. ... 9 (all set) 8 jenkins-persistent Jenkins service, with persistent storage.... 10 (all set) 8 jenkins-persistent-monitored Jenkins service, with persistent storage. ... 11 (all set) 9&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now let's try to spawn a Jenkins ephemeral report. In this case, &lt;em&gt;ephemeral&lt;/em&gt; means that the service is not storing its data. The ephemeral approach is good for a nonproduction environment. The following command creates Jenkins instances in the CI/CD namespace:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc new-app --template=jenkins-ephemeral -n cicd&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Check the created route to see the exact URL for the newly created Jenkins instances. Then open that URL and log in with your OpenShift credentials.&lt;/p&gt; &lt;h2&gt;Building and running a pipeline&lt;/h2&gt; &lt;p&gt;Start creating a build pipeline by selecting a Pipeline icon in your Jenkins instance, as shown in Figure 1.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/figure1.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/figure1.png?itok=cWjJRK96" width="600" height="140" alt="Screenshow showing how to create a Pipeline in the Jenkins console." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 1: Create a Pipeline in the Jenkins console.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Next, paste the following code into the Pipeline script, as shown in Figure 2:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;node('maven') { stage ('pull code') { sh "git clone https://github.com/edwin/hello-world-java-docker.git source" } stage ('build') { dir("source") { sh "mvn -Dmaven.repo.local=/tmp/m2 org.owasp:dependency-check-maven:check" } } stage ('generate report') { dir("source") { publishHTML (target: [ allowMissing: true, alwaysLinkToLastBuild: true, keepAll: true, reportDir: 'target', reportFiles: 'dependency-check-report.html', reportName: "Application-Dependency-Check-Report" ]) } } }&lt;/code&gt;&lt;/pre&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/figure2.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/figure2.png?itok=EjRvejxq" width="600" height="252" alt="Screenshow showing the script being inserted into a Pipeline script." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 2: The script should be inserted into a Pipeline Script.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Now focus on the "generate report" stage, where you save your HTML report as a build artifact. Press the &lt;strong&gt;Build Now&lt;/strong&gt; button, highlighted in Figure 3, to trigger the pipeline.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/figure3.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/figure3.png?itok=IyXQEsrQ" width="600" height="246" alt="Screenshow showin ghow the Pipeline's web page lets you build the Pipeline." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 3: The Pipeline's web page lets you build the Pipeline.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;And now the HTML report appears in the menu bar on the left, as shown in Figure 4.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/figure4.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/figure4.png?itok=2EPd1vFc" width="600" height="395" alt="Screenshot showing that a report appears in the menu of the Pipeline web page." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 4: A report appears in the menu of the Pipeline web page.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Clicking on the report button displays the contents of the generated HTML report, as illustrated in Figure 5.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/figure5.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/figure5.png?itok=mY8XUimA" width="600" height="388" alt="Screenshot of a formatted report displayed on the web page." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 5: A formatted report is displayed on the web page.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;If you find that your report shows some errors or is a bit disorganized, there's a workaround to fix it. Go to &lt;strong&gt;Script Console&lt;/strong&gt; inside the &lt;strong&gt;Manage Jenkins&lt;/strong&gt; menu, as shown in Figure 6.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/figure6.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/figure6.png?itok=weLqW9h1" width="600" height="231" alt="Screenshot showing that the Script Console is available in Tools and Actions." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 6: The Script Console is available in Tools and Actions.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Type the following script inside the &lt;strong&gt;Script Console&lt;/strong&gt; text field and then press the &lt;strong&gt;Run&lt;/strong&gt; button.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;System.setProperty("hudson.model.DirectoryBrowserSupport.CSP", "")&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;In addition to automating your CI/CD process, Jenkins can automate the recording of events that occur during its own run. This article has illustrated several parts of the open source environment that make it easy to generate and save reports from Jenkins.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/03/30/generate-and-save-html-report-jenkins-openshift-4" title="Generate and save an HTML report in Jenkins on OpenShift 4"&gt;Generate and save an HTML report in Jenkins on OpenShift 4&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Muhammad Edwin</dc:creator><dc:date>2022-03-30T07:00:00Z</dc:date></entry><entry><title>C++ standardization (core language) progress in 2021</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/03/29/c-standardization-core-language-progress-2021" /><author><name>Jason Merrill</name></author><id>158d176a-ef10-475d-b296-15db643e1140</id><updated>2022-03-29T07:00:00Z</updated><published>2022-03-29T07:00:00Z</published><summary type="html">&lt;p&gt;This article covers the highlights of the &lt;a href="https://developers.redhat.com/topics/c"&gt;C++&lt;/a&gt; standardization proposals before the International Organization for Standardization (ISO) committee's Core and Evolution Working Groups last year. Read on to find out what's coming in C++23.&lt;/p&gt; &lt;h2&gt;Virtual ISO working group meetings&lt;/h2&gt; &lt;p&gt;For most of 2020, meetings about the C++ standard were held virtually and treated by the attendees as tentative, mostly filling time until we could meet in person again. As in-person meetings continue to be pushed out further into the future, and we've gotten more comfortable with the online format, we've gotten a lot more done in virtual meetings.&lt;/p&gt; &lt;p&gt;Now, instead of doing most of our business during the in-person meetings three times a year, the different working groups frequently meet throughout the year, some weekly, some monthly, or whatever seems appropriate. During the weeks when we previously had our in-person meetings, we hold only the Monday plenary meeting, virtually, to ratify the work the various groups have done in the intervening months.&lt;/p&gt; &lt;h2&gt;Notable core language changes for C++23&lt;/h2&gt; &lt;p&gt;The "Declarations and where to find them" and "Down with ()" papers that I mentioned in &lt;a data-saferedirecturl="https://www.google.com/url?q=https://developers.redhat.com/blog/2021/05/07/report-from-the-virtual-iso-c-meetings-in-2020-core-language&amp;amp;source=gmail&amp;amp;ust=1641079515135000&amp;amp;usg=AOvVaw2gXTQQun-Yz6gkQR8-XYZG" href="https://developers.redhat.com/blog/2021/05/07/report-from-the-virtual-iso-c-meetings-in-2020-core-language"&gt;last year's report&lt;/a&gt; were accepted, as expected.&lt;/p&gt; &lt;h3&gt;if consteval&lt;/h3&gt; &lt;p&gt;The new &lt;code&gt;if consteval&lt;/code&gt; syntax (&lt;a data-saferedirecturl="https://www.google.com/url?q=http://wg21.link/P1938R3&amp;amp;source=gmail&amp;amp;ust=1641079515135000&amp;amp;usg=AOvVaw3_zOBnBo8UcZSqQgeGRRat" href="http://wg21.link/P1938R3"&gt;P1938R3&lt;/a&gt;) creates an immediate function context within a &lt;code&gt;constexpr&lt;/code&gt; function. In C++20. many people expected to be able to create such a context with &lt;code&gt;if (std::is_constant_evaluated())&lt;/code&gt;, but that syntax doesn't have that effect, and does have significant pitfalls. So new syntax seemed warranted. An example of the new syntax is:&lt;/p&gt; &lt;pre&gt; &lt;code class="cpp"&gt;consteval int f(int i) { return i; } constexpr int g(int i) { if consteval { return f(i) + 1; // ok: immediate function context } else { return 42; } }&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Deducing "this"&lt;/h3&gt; &lt;p&gt;&lt;a data-saferedirecturl="https://www.google.com/url?q=http://wg21.link/P0847R7&amp;amp;source=gmail&amp;amp;ust=1641079515135000&amp;amp;usg=AOvVaw3ewEtn0DkqGO_OGQIDafuA" href="http://wg21.link/P0847R7"&gt;P0847R7&lt;/a&gt; allows the object parameter (normally implicitly &lt;code&gt;this&lt;/code&gt;) of a non-static member function to be declared explicitly, and have its type deduced:&lt;/p&gt; &lt;pre&gt; &lt;code class="cpp"&gt;struct X { int i; template &lt;typename Self&gt; auto&amp;&amp; foo(this Self&amp;&amp; self) { return self.i; } };&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The primary use is expected to be to deduce the value category of the object argument, replacing the need for multiple overloads with different ref-qualifiers. One possibly surprising effect is that the object parameter type can also deduce to a derived type, causing references to a particular member to resolve instead to a member of the same name in the derived class:&lt;/p&gt; &lt;pre&gt; &lt;code class="cpp"&gt;struct D : X { char i; } d; char&amp; r = d.foo(); // ok, returns reference to D::i, not B::i &lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Adding #elifdef and #elifndef preprocessing directives&lt;/h3&gt; &lt;p&gt;These directives were recently added to C23, so &lt;a data-saferedirecturl="https://www.google.com/url?q=http://wg21.link/P2334R1&amp;amp;source=gmail&amp;amp;ust=1641079515135000&amp;amp;usg=AOvVaw22XJ-AfGfBy4VFYaBng-Ml" href="http://wg21.link/P2334R1"&gt;P2334R1&lt;/a&gt; also adds them to C++23 to avoid preprocessor incompatibilities. Programmers have often been surprised that these directives don't exist, and in C++23 they will.&lt;/p&gt; &lt;h3&gt;Multidimensional subscript operator&lt;/h3&gt; &lt;p&gt;&lt;a data-saferedirecturl="https://www.google.com/url?q=http://wg21.link/P2128R6&amp;amp;source=gmail&amp;amp;ust=1641079515135000&amp;amp;usg=AOvVaw05F4_8nThnnAQeknk4Hvt9" href="http://wg21.link/P2128R6"&gt;P2128R6&lt;/a&gt; changes the &lt;code&gt;operator[]&lt;/code&gt; member function from always having a single parameter to having any number of parameters. As a result, a multidimensional array class can use &lt;code&gt;ar[1,2,3]&lt;/code&gt; to index directly rather than through the proxy classes needed to support &lt;code&gt;ar[1][2][3]&lt;/code&gt;. Previously, the &lt;code&gt;ar[1,2,3]&lt;/code&gt; syntax was deprecated.&lt;/p&gt; &lt;h3&gt;C++ Identifier Syntax using Unicode Annex 31&lt;/h3&gt; &lt;p&gt;Past C++ standards have attempted to define their own subsets of Unicode to allow in identifiers, but each revision had flaws. C++23 (&lt;a href="http://wg21.link/"&gt;P1949R7&lt;/a&gt;) leaves that task instead to the Unicode Consortium, which now provides &lt;a data-saferedirecturl="https://www.google.com/url?q=http://www.unicode.org/reports/tr31/&amp;amp;source=gmail&amp;amp;ust=1641079515135000&amp;amp;usg=AOvVaw1u5OV0QMDLlf0PJ7bLq9_i" href="http://www.unicode.org/reports/tr31/"&gt;Annex 31&lt;/a&gt; for this purpose. This change conveniently coincides with the recent concern about &lt;a data-saferedirecturl="https://www.google.com/url?q=https://access.redhat.com/security/vulnerabilities/RHSB-2021-007&amp;amp;source=gmail&amp;amp;ust=1641079515135000&amp;amp;usg=AOvVaw1RrkyUw__nni5VJuzYCvIt" href="https://access.redhat.com/security/vulnerabilities/RHSB-2021-007"&gt;Trojan source attacks using bidirectional text&lt;/a&gt;, because Annex 31 significantly limits the use of bidirectional characters in identifiers.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note:&lt;/strong&gt; Find out &lt;a href="https://developers.redhat.com/articles/2022/01/12/prevent-trojan-source-attacks-gcc-12"&gt;how to prevent Trojan Source attacks with GCC 12&lt;/a&gt;.&lt;/p&gt; &lt;h3&gt;CWG2397, auto specifier for pointers and references to arrays&lt;/h3&gt; &lt;p&gt;&lt;a data-saferedirecturl="https://www.google.com/url?q=http://wg21.link/P2386R0&amp;amp;source=gmail&amp;amp;ust=1641079515135000&amp;amp;usg=AOvVaw3pZmbKWjysVeEbYVsajeVX" href="http://wg21.link/P2386R0"&gt;P2386R0&lt;/a&gt; allows the declaration of pointers and references to arrays of &lt;code&gt;auto&lt;/code&gt;, such as:&lt;/p&gt; &lt;pre&gt; &lt;code class="cpp"&gt; int a[3]; auto (*p)[3] = &amp;a; // now OK&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Non-literal variables, labels, and gotos in constexpr functions&lt;/h3&gt; &lt;p&gt;Restrictions on &lt;code&gt;constexpr&lt;/code&gt; functions have been gradually weakening since C++11, in particular by changing requirements that a particular construct never appear in the function; instead the construct can be used but makes the evaluation non-constant. &lt;a data-saferedirecturl="https://www.google.com/url?q=http://wg21.link/P2242R3&amp;amp;source=gmail&amp;amp;ust=1641079515135000&amp;amp;usg=AOvVaw0dRp-YrPXlhKN8u2a3KGT9" href="http://wg21.link/P2242R3"&gt;P2242R3&lt;/a&gt; makes that change for the constructs mentioned in the title, and for static or thread-local variables.&lt;/p&gt; &lt;h2&gt;What's in the pipeline for C++23?&lt;/h2&gt; &lt;p&gt;In addition to the changes just described, which the committee has accepted, additional changes are likely to make it into C++23. At least they have moved on to electronic voting by the full Evolution Working Group.&lt;/p&gt; &lt;h3&gt;Attributes on lambda-expressions&lt;/h3&gt; &lt;p&gt;There has not yet been a way to apply an attribute like &lt;code&gt;[[nodiscard]]&lt;/code&gt; to a lambda's operator(). The proposed &lt;a data-saferedirecturl="https://www.google.com/url?q=http://wg21.link/P2173R1&amp;amp;source=gmail&amp;amp;ust=1641079515135000&amp;amp;usg=AOvVaw1rz_Vj_S5_HlZZ3a-mJFFQ" href="http://wg21.link/P2173R1"&gt;P2173R1&lt;/a&gt; uses the syntax &lt;code&gt;[][[nodiscard]]() { ... }&lt;/code&gt;, which matches the position of attributes that appertain to a declarator-id in a declaration that has one.&lt;/p&gt; &lt;h3&gt;The equality operator you are looking for&lt;/h3&gt; &lt;p&gt;&lt;a data-saferedirecturl="https://www.google.com/url?q=http://wg21.link/P2468&amp;amp;source=gmail&amp;amp;ust=1641079515135000&amp;amp;usg=AOvVaw3wxSFmc3TLae5E9noUlfLp" href="http://wg21.link/P2468"&gt;P2468&lt;/a&gt; attempts to fix a long-standing problem. The implicit reversal of &lt;code&gt;operator==&lt;/code&gt; made some valid C++17 code ill-formed in C++20, most frequently where a class defines comparison operators that are accidentally asymmetric:&lt;/p&gt; &lt;pre&gt; &lt;code class="cpp"&gt;struct S { bool operator==(const S&amp;) { return true; } // mistakenly non-const bool operator!=(const S&amp;) { return false; } // mistakenly non-const }; bool b = S{} != S{}; // well-formed in C++17, ambiguous in C++20&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In the GCC compiler, I made this example work by making a reversed operator worse than an unreversed operator if they have the same parameter types. P2468 considered this fix, but instead proposes that if a class has matching &lt;code&gt;operator==&lt;/code&gt; and &lt;code&gt;operator!=&lt;/code&gt; declarations, no reversed candidates are considered.&lt;/p&gt; &lt;h3&gt;More constexpr relaxations&lt;/h3&gt; &lt;p&gt;In C++20, a function declared &lt;code&gt;constexpr&lt;/code&gt; that can never produce a constant result is ill-formed, and no diagnostic is required. This has proven a burden for implementers who would like their function to be usable in a constant expression when other functions that it calls become usable, but don't want to have to use macro trickery to coordinate the addition of the &lt;code&gt;constexpr&lt;/code&gt; keyword at the same time. So &lt;a data-saferedirecturl="https://www.google.com/url?q=http://wg21.link/P2448&amp;amp;source=gmail&amp;amp;ust=1641079515135000&amp;amp;usg=AOvVaw1QRy4t6DUh_7sUWTyhYaWM" href="http://wg21.link/P2448"&gt;P2448&lt;/a&gt; proposes removing that rule, along with the related rule that a defaulted member function can be declared &lt;code&gt;constexpr&lt;/code&gt; only if it could produce a constant result.&lt;/p&gt; &lt;p&gt;&lt;a data-saferedirecturl="https://www.google.com/url?q=http://wg21.link/P2350&amp;amp;source=gmail&amp;amp;ust=1641079515135000&amp;amp;usg=AOvVaw1zaBq7Q0D9NQeSnNLHbCXH" href="http://wg21.link/P2350"&gt;P2350&lt;/a&gt; proposes allowing a class to be marked as constexpr to avoid the need to mark each member function separately.&lt;/p&gt; &lt;p&gt;&lt;a data-saferedirecturl="https://www.google.com/url?q=http://wg21.link/P2280&amp;amp;source=gmail&amp;amp;ust=1641079515135000&amp;amp;usg=AOvVaw1xsO6OOLdDGKsyjv0UDzkT" href="http://wg21.link/P2280"&gt;P2280&lt;/a&gt; proposes that binding an object with a non-constant address to a reference parameter of a &lt;code&gt;constexpr&lt;/code&gt; function be allowed in a constant-expression, so long as it is never accessed. This is intended particularly to support uses like the following:&lt;/p&gt; &lt;pre&gt; &lt;code class="cpp"&gt;template &lt;typename T, size_t N&gt; constexpr size_t array_size(T (&amp;)[N]) { return N; // referent of parameter is never used }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Inspired by these changes, for GCC 12, I've added an &lt;code&gt;-fimplicit-constexpr&lt;/code&gt; option that takes what seems to me the natural next step of treating all inline functions as implicitly &lt;code&gt;constexpr&lt;/code&gt;; I'm interested in feedback about people's experience with it before I propose it formally.&lt;/p&gt; &lt;h3&gt;Extended floating-point types and standard names&lt;/h3&gt; &lt;p&gt;&lt;a data-saferedirecturl="https://www.google.com/url?q=http://wg21.link/P1467&amp;amp;source=gmail&amp;amp;ust=1641079515135000&amp;amp;usg=AOvVaw16uFucfFjKMZeJR6MSSfRi" href="http://wg21.link/P1467"&gt;P1467&lt;/a&gt; adds conditionally-supported &lt;code&gt;std::float{16,32,64,128}_t&lt;/code&gt; and &lt;code&gt;std::bfloat16_t&lt;/code&gt; for the corresponding IEEE types, as well as corresponding literal suffixes and adjustments to overload resolution to rank conversions between the greatly expanded set of floating-point types.&lt;/p&gt; &lt;h3&gt;Static operator()&lt;/h3&gt; &lt;p&gt;It seems unnecessary to prohibit the &lt;code&gt;operator()&lt;/code&gt; member function from being static, and &lt;a data-saferedirecturl="https://www.google.com/url?q=http://wg21.link/P1169&amp;amp;source=gmail&amp;amp;ust=1641079515135000&amp;amp;usg=AOvVaw3skaAVqWz78W-9_bZKj-Fx" href="http://wg21.link/P1169"&gt;P1169&lt;/a&gt; allows it.  If a static definition had been allowed when lambdas were introduced, the call operator for a captureless lambda probably would have been implicitly static, but this paper does not propose making that change.&lt;/p&gt; &lt;h3&gt;Support for #warning&lt;/h3&gt; &lt;p&gt;&lt;a data-saferedirecturl="https://www.google.com/url?q=http://wg21.link/P2437&amp;amp;source=gmail&amp;amp;ust=1641079515135000&amp;amp;usg=AOvVaw1aPG6otqkfyA2wLBpiKKxB" href="http://wg21.link/P2437"&gt;P2437&lt;/a&gt; adds the #warning preprocessor directive. Most compilers have already supported it for a long time.&lt;/p&gt; &lt;h3&gt;Labels at the end of compound statements&lt;/h3&gt; &lt;p&gt;&lt;a data-saferedirecturl="https://www.google.com/url?q=http://wg21.link/P2324&amp;amp;source=gmail&amp;amp;ust=1641079515135000&amp;amp;usg=AOvVaw2nkm21KFkIZ_7nbHJLNh4i" href="http://wg21.link/P2324"&gt;P2324&lt;/a&gt; makes a convenience tweak that has already been adopted by C.&lt;/p&gt; &lt;h3&gt;Portable assumptions&lt;/h3&gt; &lt;p&gt;&lt;a data-saferedirecturl="https://www.google.com/url?q=http://wg21.link/P1774&amp;amp;source=gmail&amp;amp;ust=1641079515135000&amp;amp;usg=AOvVaw2OMIBVhk69E0PYOUsyIkEe" href="http://wg21.link/P1774"&gt;P1774&lt;/a&gt; adds the &lt;code&gt;[[assume(expr)]]&lt;/code&gt; syntax to tell the compiler that it should assume during optimization that the expression argument evaluates to true. The C++20 contracts proposal ran aground on disagreement over whether contracts should or should not be assumed, so this proposal is now independent of any potential future contracts feature. This might be the least likely of the features discussed here to make it into C++23, as there is significant resistance to any assumption feature from one vendor, but there was strong support from the rest of the evolution group when the paper was discussed.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;Work continues in study groups on various other proposals, such as pattern matching (&lt;a data-saferedirecturl="https://www.google.com/url?q=http://wg21.link/P1371&amp;amp;source=gmail&amp;amp;ust=1641079515135000&amp;amp;usg=AOvVaw3JQRHNSR80pmtB8JI2OUfW" href="http://wg21.link/P1371"&gt;P1371&lt;/a&gt;) and a return of Contracts (&lt;a data-saferedirecturl="https://www.google.com/url?q=http://wg21.link/P2182&amp;amp;source=gmail&amp;amp;ust=1641079515135000&amp;amp;usg=AOvVaw30kCxjhgzF9PbsT46Nz8sq" href="http://wg21.link/P2182"&gt;P2182&lt;/a&gt;), but they are not trying to hit the deadline for C++23.&lt;/p&gt; &lt;p&gt;At the moment, the &lt;a data-saferedirecturl="https://www.google.com/url?q=https://isocpp.org/std/meetings-and-participation/upcoming-meetings&amp;amp;source=gmail&amp;amp;ust=1641079515135000&amp;amp;usg=AOvVaw0Nrhg62bi8B5urQofUPXSu" href="https://isocpp.org/std/meetings-and-participation/upcoming-meetings"&gt;next in-person meeting&lt;/a&gt; is planned for November 2022. We'll see whether it actually happens, and if so, how it is affected by our experience with virtual meetings.&lt;/p&gt; &lt;p&gt;Many of the proposed changes in C++23 simplify or relax complex restrictions. The changes reflect requests from the field and should make C++ easier for complex projects.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/03/29/c-standardization-core-language-progress-2021" title="C++ standardization (core language) progress in 2021"&gt;C++ standardization (core language) progress in 2021&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Jason Merrill</dc:creator><dc:date>2022-03-29T07:00:00Z</dc:date></entry><entry><title type="html">Portfolio Architecture Examples - Telco Collection</title><link rel="alternate" href="http://www.schabell.org/2022/03/portfolio-architecture-exmaples-telco-collection.html" /><author><name>Eric D. Schabell</name></author><id>http://www.schabell.org/2022/03/portfolio-architecture-exmaples-telco-collection.html</id><updated>2022-03-29T05:00:00Z</updated><content type="html">Figure 1: The portfolio architecture process For a few years now we've been working on a project we have named . These are based on selecting a specific use case we are seeing used in the real world by customers and then finding implementations of that case using three or more products from the Red Hat portfolio. This basic premise is used as the foundation, but many aspects of open source are included in both the process and the final product we have defined. There is a community, where we share the initial project kickoff with a group of architects and use their initial feedback from the start. We also present the architecture product we've created right at the end before we publish to ensure usability by architects in the field. The final publish product includes some internal only content around the customer projects researched, but most of the content is  through various open source channels.  This article is sharing an overview of the product we've developed, what's available to you , and concludes by sharing a collection of architectures we've published. INTRODUCTION The basis of a portfolio architecture is a use case, two to three actual implementations that can be researched, and includes the use of a minimum of three products. This is the ideal foundation for a project to start, but we encountered a problem with use cases containing emerging technologies or emerging domains in the market. To account for these we've chosen to note the fact that these are opinionated architectures based on internal reference architectures.  The product has been defined as complete for publishing when it contains the following content: * Short use case definition * Diagrams - logical, schematic (physical), and detail diagrams * Public slide deck containing the use case story and architecture diagrams * Internal slide deck containing both the pubic deck content and the confidential customer research * Video (short) explanation of the architecture * Either a technical brief document or one or more articles covering the solution architecture Note that the above items noted in italics are all freely available to you online in the Red Hat Portfolio Architecture Center or in the Portfolio Architecture Examples repository. FIGURE 2: LOGICAL DIAGRAM DESIGN TEMPLATE TOOLING AND WORKSHOPS The progress towards our products required a good idea of how we wanted to diagram our architectures. We chose to keep them very generic and simple in style to facilitate all levels of conversation around a particular use case without getting bogged down in notational discussions.  A simple three level design for our architectures was captured by using logical, schematic, and detail diagrams. All of these have been integrated in  with pre-defined templates and icons for easily getting started. Furthermore, we've developed a tooling workshop to quickly ramp up on the design methods and tooling we've made available. It's called , has been featured in several. TELCO COLLECTION The collection featured today is centred around architectures in the telco domain. There are currently three architectures in this collection and we'll provide a short overview of each, leaving the in depth exploration as an exercise for the reader. Figure 3: Telco architecture collection In each of these architecture overviews you'll find a table of contents outlining the technologies used, several example schematic diagrams with descriptions, and a link in the last section to open the diagrams directly into the online tooling in your browser. 5G is the latest evolution of wireless mobile technology that aims to enable the delivery of highly immersive experiences for people and ultra reliable, low latency communication between devices. At the heart of each 5G network lies the 5G Core (5GC). This portfolio architecture conceives 5G Core as a set of disaggregated, cloud native applications that communicate internally and externally over well defined standard interfaces. Each 5GC component is implemented as a container-based application and is referred to as cloud-native network function (CNF). The use case is ultra-reliable, immersive experiences for people and objects when and where it matters most. 5G is the latest evolution of wireless mobile technology. In this portfolio architecture we’ll discuss an 5G solution built with open source technologies at core, that can work across any hyperscaler. The use case is building an adaptable, on-demand infrastructure services for 5G Core that can deliver across diverse use cases with minimal CAPEX and OPEX. 5G is the latest evolution of wireless mobile technology. It can deliver a number of services from the network edge: * Enhanced mobile broadband (eMBB) * 5G enhances data speeds and experiences with new radio capabilities like mmWave frequency spectrum for higher bandwidth allocation and theoretical throughput up to 20Gbps. * Ultra-reliable, low-latency communication (uRLLC) * 5G supports vertical industry requirements, including sub-millisecond latency with less than 1 lost packet in 105 packets. * Massive machine type communications (mMTC) * 5G supports cost-efficient and robust connection for up to 1 million mMTC, NB-IOT, and LTE-M devices per square kilometer without network overloading. The use case is base on the facts that digital transformation of mobile networks is accelerating and cloudification is increasing. Following the core network, radio access network (RAN) solutions are now taking advantage of the benefits of cloud computing. If you are interested in more architecture solutions like these, feel free to export the . More architecture collections include: * Application development * Automation * Data engineering * Edge * Finance * * Infrastructure * Manufacturing * Retail * * Utility</content><dc:creator>Eric D. Schabell</dc:creator></entry><entry><title type="html">WildFly 26.1 Beta S2I images have been released on quay.io</title><link rel="alternate" href="https://wildfly.org//news/2022/03/29/WildFly-s2i-26-1-Beta1-Released/" /><author><name>Jean-François Denise</name></author><id>https://wildfly.org//news/2022/03/29/WildFly-s2i-26-1-Beta1-Released/</id><updated>2022-03-29T00:00:00Z</updated><content type="html">WILDFLY 26.1 BETA S2I DOCKER IMAGES The WildFly S2I (Source-to-Image) builder and runtime Docker images for WildFly 26.1 Beta have been released on . For complete documentation on how to use these images using S2I, OpenShift and Docker, refer to the WildFly S2I . ANTICIPATING A FUTURE SET OF WILDFLY IMAGES The quay.io/wildfly/wildfly-centos7 and quay.io/wildfly/wildfly-runtime-centos7 have been deprecated since WildFly 26. When building or running the server, a deprecation notice is displayed in the console. This WildFly 26.1 Beta and the following 26.1 Final images will be the latest released in quay.io. We have started our migration to the new WildFly S2I images that will be released for WildFly 26.1. The new architecture is based on the and a new pair of docker images. This has provided the details of this new approach. Stay tuned! JF Denise</content><dc:creator>Jean-François Denise</dc:creator></entry><entry><title>Simplify secure connections to PostgreSQL databases with Node.js</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/03/28/simplify-secure-connections-postgresql-databases-nodejs" /><author><name>Michael Dawson</name></author><id>f045aeec-19ed-4a10-a365-a0fcbfed01d6</id><updated>2022-03-28T07:00:00Z</updated><published>2022-03-28T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://www.postgresql.org/"&gt;PostgreSQL&lt;/a&gt; is an advanced open source relational database that is commonly used by applications to store structured data. Before accessing a database, the application must connect and provide security credentials. As a &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;Node.js&lt;/a&gt; developer, how can you safely share and provide those credentials in &lt;a href="https://developers.redhat.com/topics/javascript"&gt;JavaScript&lt;/a&gt; code without a lot of work? This article introduces service bindings and the &lt;a href="https://www.npmjs.com/package/kube-service-bindings"&gt;kube-service-bindings&lt;/a&gt; package, along with a convenient graphical interface in &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;When using a database, the four basic operations are create, read, update, and delete (CRUD, for short). Our team maintains an &lt;a href="https://github.com/nodeshift-starters/nodejs-rest-http-crud"&gt;example CRUD application on GitHub&lt;/a&gt; that shows how to connect to a PostgreSQL database and execute the four basic operations. We use that example to illustrate the security model in this article.&lt;/p&gt; &lt;h2&gt;Security risks when connecting to the PostgreSQL database&lt;/h2&gt; &lt;p&gt;The information you need to connect to a PostgreSQL database is:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;User&lt;/li&gt; &lt;li&gt;Password&lt;/li&gt; &lt;li&gt;Host&lt;/li&gt; &lt;li&gt;Database&lt;/li&gt; &lt;li&gt;Port&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;You definitely need to be careful about who has access to the user and password, and ideally, you don't want any of these values to be public. This section looks at some simple methods that fail to protect this sensitive information adequately.&lt;/p&gt; &lt;h3&gt;Setting environment variables explicitly&lt;/h3&gt; &lt;p&gt;Using environment variables is the easiest way to configure a connection and is often used in examples like the following JavaScript code:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-javascript"&gt;const serviceHost = process.env.MY_DATABASE_SERVICE_HOST; const user = process.env.DB_USERNAME; const password = process.env.DB_PASSWORD; const databaseName = process.env.POSTGRESQL_DATABASE const connectionString = `postgresql://${user}:${password}@${serviceHost}:5432/${databaseName}`; connectionOptions = { connectionString }; const pool = new Pool(connectionOptions);&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Unfortunately, using environment variables is not necessarily secure. If you set the environment variables from the command line, anybody with access to the environment can see them. Tools and frameworks also often make it easy to access environment variables for debugging purposes. For example, in OpenShift, you can view the environment variables from the console, as shown in Figure 1. So you need to find a way to provide connection credentials while keeping them hidden from interlopers.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/pod_0.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/pod_0.png?itok=gwHLAigv" width="824" height="642" alt="Pod details in the OpenShift console reveal the environmental variables set in the pod." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 1: Pod details in the OpenShift console reveal the environmental variables set in the pod.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;Loading environment variables from dotenv&lt;/h3&gt; &lt;p&gt;Instead of setting the credentials in the environment directly, a safer way is to use a package such as &lt;a href="https://www.npmjs.com/package/dotenv"&gt;dotenv&lt;/a&gt; to get the credentials from a file and provide them to the Node.js application environment. The benefit of using &lt;code&gt;dotenv&lt;/code&gt; is that the credentials don't show up in the environment outside of the Node.js process.&lt;/p&gt; &lt;p&gt;Although this approach is better, the credentials still might be exposed if you dump the Node.js environment for debugging through a &lt;a href="https://developer.ibm.com/articles/easily-identify-problems-in-your-nodejs-apps-with-diagnostic-report/"&gt;Node.js diagnostic report&lt;/a&gt;. You are also left with the question of how to get the &lt;code&gt;dotenv&lt;/code&gt; file securely to the application. If you are deploying to &lt;a href="https://developers.redhat.com/topics/kubernetes/"&gt;Kubernetes&lt;/a&gt;, you can map a file into deployed &lt;a href="https://developers.redhat.com/topics/containers/"&gt;containers&lt;/a&gt;, but that will take some planning and coordination for deployments.&lt;/p&gt; &lt;p&gt;By this point, you are probably thinking that this seems like a lot of work and are wondering whether you need to configure the connection information for each type of service and set of credentials that are needed by an application. The good news is that for Kubernetes environments, this problem has already been solved. We cover the solution, service binding, in the next section.&lt;/p&gt; &lt;h2&gt;Passing the credentials securely: Service binding in Kubernetes&lt;/h2&gt; &lt;p&gt;Service binding is a standard approach to map a set of files into containers to provide credentials in a safe and scalable way. You can read more about the Service Binding specification for Kubernetes on &lt;a href="https://github.com/k8s-service-bindings/spec"&gt;GitHub&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;The specification does not define what files are mapped in for a given service type. In OpenShift, binding to a PostgreSQL database instance (created using either the Crunchy or the Cloud Native PostgreSQL Operators, as described in an &lt;a href="https://developers.redhat.com/articles/2021/10/27/announcing-service-binding-operator-10-ga"&gt;overview of the Service Binding Operator&lt;/a&gt;) results in mapping the following files into the application container:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ SERVICE_BINDING_ROOT/&lt;postgressql-instance-name&gt; ├── user ├── host ├── database ├── password ├── port ├── ca.crt └── tls.key └── tls.crt&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;code&gt;SERVICE_BINDING_ROOT&lt;/code&gt; is passed to the application through the environment.&lt;/p&gt; &lt;p&gt;The last three files contain the keys and certificates needed to connect over the widely used Transport Layer Security (TLS) standard and are present only if the database is configured to use TLS.&lt;/p&gt; &lt;h2&gt;Consuming service bindings easily with kube-service-bindings&lt;/h2&gt; &lt;p&gt;Now that you have the credentials available to the application running in the container, the remaining work is to read the credentials from those files and provide them to the PostgreSQL client used within your Node.js application. But wait—that still sounds like a lot of work, and it's also tied to the client you are using.&lt;/p&gt; &lt;p&gt;To make this easier, we've put together an npm package called &lt;a href="https://www.npmjs.com/package/kube-service-bindings"&gt;kube-service-bindings&lt;/a&gt;, which makes it easy for Node.js applications to consume these secrets without requiring developers to be familiar with service bindings.&lt;/p&gt; &lt;p&gt;The package provides the &lt;code&gt;getBinding()&lt;/code&gt; method, which does roughly the following:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Look for the &lt;code&gt;SERVICE_BINDING_ROOT&lt;/code&gt; variable in order to determine whether bindings are available.&lt;/li&gt; &lt;li&gt;Read the connection information from the files.&lt;/li&gt; &lt;li&gt;Map the names of the files to the option names needed by the Node.js clients that will connect to the service.&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;Figure 2 shows the steps.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/get.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/get.png?itok=DwFwUD5F" width="1191" height="707" alt="The getBinding() method involves three main steps." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 2: The getBinding() method involves three main steps.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Let's assume you connect to PostgreSQL using the popular &lt;a href="https://www.npmjs.com/package/pg"&gt;pg&lt;/a&gt; client, a library that provides all the basic commands to interact with the database. In this case you call the &lt;code&gt;getBinding()&lt;/code&gt; method with &lt;code&gt;POSTGRESQL&lt;/code&gt; and &lt;code&gt;pg&lt;/code&gt; to tell &lt;code&gt;kube-service-bindings&lt;/code&gt; which client the application is using, and then pass the object returned by &lt;code&gt;getBinding()&lt;/code&gt;when you create a Pool object. Minus error checking, the code is as simple as this:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-javascript"&gt;const serviceBindings = require('kube-service-bindings'); const { Pool } = require('pg'); let connectionOptions; try { connectionOptions = serviceBindings.getBinding('POSTGRESQL', 'pg'); } catch (err) { } const pool = new Pool(connectionOptions);&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The first parameter to &lt;code&gt;getBindings()&lt;/code&gt; is &lt;code&gt;POSTGRESQL&lt;/code&gt;, to specify that you are connecting to a PostgreSQL database. The second parameter, &lt;code&gt;pg&lt;/code&gt;, tells &lt;code&gt;kube-service-bindings&lt;/code&gt; that you are using the &lt;code&gt;pg&lt;/code&gt; client so that the call will return the information as an object that can be passed when creating a &lt;code&gt;pg&lt;/code&gt; Pool object.&lt;/p&gt; &lt;p&gt;The CRUD example, and more specifically the &lt;a href="https://github.com/nodeshift-starters/nodejs-rest-http-crud/blob/94cce5e4056f58511bd1b66576e64594e5ac4d4f/lib/db/index.js#L7"&gt;lib/db/index.js&lt;/a&gt; file, has been updated so that it can get the credentials from the environment, or automatically using &lt;code&gt;kube-service-bindings&lt;/code&gt; when credentials are available through service bindings.&lt;/p&gt; &lt;p&gt;With &lt;code&gt;kube-service-bindings&lt;/code&gt;, it's easy for Node.js developers to use credentials available through service bindings. The second part is to set up the service bindings themselves. The procedure is to install the Service Binding Operator as &lt;a href="https://developers.redhat.com/articles/2021/10/27/announcing-service-binding-operator-10-ga"&gt;described in the overview article&lt;/a&gt; mentioned earlier, install an Operator to help you create databases, create the database for your application, and finally apply some YAML to tell the Service Binding Operator to bind the database to your application.&lt;/p&gt; &lt;h2&gt;Setting up service bindings in OpenShift&lt;/h2&gt; &lt;p&gt;With the release of &lt;a href="https://docs.openshift.com/container-platform/4.8/release_notes/ocp-4-8-release-notes.html"&gt;OpenShift 4.8&lt;/a&gt;, you can use the OpenShift user interface (UI) to do the service binding. Thus, administrators and operators of a cluster can easily set up the PostgreSQL database instance for an organization. Developers can then connect their applications without needing to know the credentials. You can use the UI for convenience during initial development, and then YAML for more automated or production deployments.&lt;/p&gt; &lt;p&gt;The UI steps are quite simple:&lt;/p&gt; &lt;ol&gt;&lt;li&gt; &lt;p&gt;Create a database using one of the PostgresSQL Operators.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Deploy your application to the same namespace using &lt;code&gt;kube-service-bindings&lt;/code&gt;. Figure 3 shows the topology view of the namespace.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/name_0.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/name_0.png?itok=Kdcvsl97" width="600" height="244" alt="The namespace contains the PostgreSQL database and Node.js application." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 3: The namespace contains the PostgreSQL database and Node.js application.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/li&gt; &lt;li&gt; &lt;p&gt;Drag a link from the application to the database until you see the "Create a binding connector" box pop up (Figure 4).&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/bind.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/bind.png?itok=nH27ra3j" width="600" height="249" alt="Create a binding from the Node.js application to the PostgreSQL database." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 4: Create a binding from the Node.js application to the PostgreSQL database.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/li&gt; &lt;li&gt; &lt;p&gt;Finally, release the mouse button. The binding is created (Figure 5) and the credentials are automatically mapped into your application pods. If you've configured your application to retry the connection until service bindings are available, it should then get the credentials and connect to the database.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/done.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/done.png?itok=iDfeRLWt" width="600" height="266" alt="The binding is established." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 5: The binding is established.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/li&gt; &lt;/ol&gt;&lt;h2&gt;Further resources&lt;/h2&gt; &lt;p&gt;This article introduced you to the credentials needed to connect to a PostgreSQL database and how they can be safely provided to your Node.js applications. To learn more, try the following:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Install and experiment with the &lt;a href="https://github.com/nodeshift-starters/nodejs-rest-http-crud"&gt;CRUD example&lt;/a&gt; to explore the code and &lt;a href="https://www.npmjs.com/package/kube-service-bindings"&gt;kube-service-bindings&lt;/a&gt;. (If you are really adventurous, you can create your own files and set &lt;code&gt;SERVICE_BINDING_ROOT&lt;/code&gt; to point to them.)&lt;/li&gt; &lt;li&gt;Work through how to set up service bindings for a PostgreSQL database using the instructions in the &lt;a href="https://developers.redhat.com/articles/2021/10/27/announcing-service-binding-operator-10-ga"&gt;Service Binding Operator overview&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Connect the &lt;a href="https://github.com/nodeshift-starters/nodejs-rest-http-crud"&gt;CRUD example&lt;/a&gt; to the PostgreSQL database you created using the UI.&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;We hope you found this article informative. To stay up to date with what else Red Hat is up to on the Node.js front, check out our &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;Node.js topic page&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/03/28/simplify-secure-connections-postgresql-databases-nodejs" title="Simplify secure connections to PostgreSQL databases with Node.js"&gt;Simplify secure connections to PostgreSQL databases with Node.js&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Michael Dawson</dc:creator><dc:date>2022-03-28T07:00:00Z</dc:date></entry><entry><title>Quarkus Tools for Visual Studio Code - 1.10.0 release</title><link rel="alternate" href="&#xA;                https://quarkus.io/blog/vscode-quarkus-1.10.0/&#xA;            " /><author><name>Roland Grunberg</name></author><id>https://quarkus.io/blog/vscode-quarkus-1.10.0/</id><updated>2022-03-28T00:00:00Z</updated><published>2022-03-28T00:00:00Z</published><summary type="html">Quarkus Tools for Visual Studio Code 1.10.0 has been released on the VS Code Marketplace &amp; Open VSX. For a list of all changes, please refer to the changelog. It’s been about 8 months since our last release of Quarkus Tools for VS Code. 8 months!! That’s a really long...</summary><dc:creator>Roland Grunberg</dc:creator><dc:date>2022-03-28T00:00:00Z</dc:date></entry><entry><title type="html">Application modernization patterns with Apache Kafka, Debezium, and Kubernetes</title><link rel="alternate" href="http://www.ofbizian.com/2022/03/application-modernization-patterns-with.html" /><author><name>Unknown</name></author><id>http://www.ofbizian.com/2022/03/application-modernization-patterns-with.html</id><updated>2022-03-27T11:23:00Z</updated><content type="html">“We build our computers the way we build our cities—over time, without a plan, on top of ruins.” Ellen Ullman , but it applies just as much today to the way we build modern applications; that is, over time, with short-term plans, on top of legacy software. In this article, I will introduce a few patterns and tools that I believe work well for thoughtfully modernizing legacy applications and building modern event-driven systems. Note: If you prefer watch my talk on YouTube about which this post is  based on. APPLICATION MODERNIZATION IN CONTEXT Application modernization refers to the process of taking an existing legacy application and modernizing its infrastructure—the internal architecture—to improve the velocity of new feature delivery, improve performance and scalability, expose the functionality for new use cases, and so on. Luckily, there is already a good classification of modernization and migration types, as shown in Figure 1. Figure 1: Three modernization types and the technologies we might use for them. Depending on your needs and appetite for change, there are a few levels of modernization: * Retention: The easiest thing you can do is to retain what you have and ignore the application's modernization needs. This makes sense if the needs are not yet pressing. * Retirement: Another thing you could do is retire and get rid of the legacy application. That is possible if you discover the application is no longer being used. * Rehosting: The next thing you could do is to rehost the application, which typically means taking an application as-is and hosting it on new infrastructure such as cloud infrastructure, or even on Kubernetes through something like KubeVirt. This is not a bad option if your application cannot be containerized, but you still want to reuse your Kubernetes skills, best practices, and infrastructure to . * Replatforming: When changing the infrastructure is not enough and you are doing a bit of alteration at the edges of the application without changing its architecture, replatforming is an option. Maybe you are changing the way the application is configured so that it can be containerized, or moving from a legacy Java EE runtime to an open source runtime. Here, you could use a tool like to analyze your application and return a report with what needs to be done. * Refactoring: Much application modernization today focuses on migrating monolithic, on-premises applications to a cloud-native microservices architecture that supports faster release cycles. That involves refactoring and rearchitecting your application, which is the focus of this article. For this article, we will assume we are working with a monolithic, on-premise application, which is a common starting point for modernization. The approach discussed here could also apply to other scenarios, such as a cloud migration initiative. CHALLENGES OF MIGRATING MONOLITHIC LEGACY APPLICATIONS Deployment frequency is a common challenge for migrating monolithic legacy applications. Another challenge is scaling development so that more developers and teams can work on a common code base without stepping on each other’s toes. Scaling the application to handle an increasing load in a reliable way is another concern. On the other hand, the expected benefits from a modernization include reduced time to market, increased team autonomy on the codebase, and dynamic scaling to handle the service load more efficiently. Each of these benefits offsets the work involved in modernization. Figure 2 shows an example infrastructure for scaling a legacy application for increased load. Figure 2: Refactoring a legacy application into event-driven microservices. ENVISIONING THE TARGET STATE AND MEASURING SUCCESS For our use case, the target state is an architectural style that follows microservices principles using open source technologies such as Kubernetes, , and Debezium. We want to end up with independently deployable services modeled around a business domain. Each service should own its own data, emit its own events, and so on. When we plan for modernization, it is also important to consider how we will measure the outcomes or results of our efforts. For that purpose, we can use metrics such as lead time for changes, deployment frequency, time to recovery, concurrent users, and so on. The next sections will introduce three design patterns and three open source technologies—Kubernetes, Apache Kafka, and Debezium—that you can use to migrate from brown-field systems toward green-field, modern, event-driven services. We will start with the Strangler pattern. THE STRANGLER PATTERN The Strangler pattern is the most popular technique used for application migrations. Martin Fowler introduced and popularized this pattern under the name of , which was inspired by a type of fig that seeds itself in the upper branches of a tree and gradually evolves around the original tree, eventually replacing it. The parallel with application migration is that our new service is initially set up to wrap the existing system. In this way, the old and the new systems can coexist, giving the new system time to grow and potentially replace the old system. Figure 3 shows the main components of the Strangler pattern for a legacy application migration. Figure 3: The Strangler pattern in a legacy application migration. The key benefit of the Strangler pattern is that it allows low-risk, incremental migration from a legacy system to a new one. Let’s look at each of the main steps involved in this pattern. STEP 1: IDENTIFY FUNCTIONAL BOUNDARIES The very first question is where to start the migration. Here, we can use domain-driven design to help us identify aggregates and the bounded contexts where each represents a potential unit of decomposition and a potential boundary for microservices. Or, we can use the technique created by Antonio Brandolini to gain a shared understanding of the domain model. Other important considerations here would be how these models interact with the database and what work is required for database decomposition. Once we have a list of these factors, the next step is to identify the relationships and dependencies between the bounded contexts to get an idea of the relative difficulty of the extraction. Armed with this information, we can proceed with the next question: Do we want to start with the service that has the least amount of dependencies, for an easy win, or should we start with the most difficult part of the system? A good compromise is to pick a service that is representative of many others and can help us build a good technology foundation. That foundation can then serve as a base for estimating and migrating other modules. STEP 2: MIGRATE THE FUNCTIONALITY For the strangler pattern to work, we must be able to clearly map inbound calls to the functionality we want to move. We must also be able to redirect these calls to the new service and back if needed. Depending on the state of the legacy application, client applications, and other constraints, weighing our options for this interception might be straightforward or difficult: * The easiest option would be to change the client application and redirect inbound calls to the new service. Job done. * If the legacy application uses HTTP, then we’re off to a good start. HTTP is very amenable to redirection and we have a wealth of transparent proxy options to choose from. * In practice, it likely that our application will not only be using REST APIs, but will have SOAP, FTP, RPC, or some kind of traditional messaging endpoints, too. In this case, we may need to build a custom protocol translation layer with something like . Interception is a potentially dangerous slippery slope: If we start building a custom protocol translation layer that is shared by multiple services, we risk adding too much intelligence to the shared proxy that services depend on. This would move us away from the "” mantra. A better option is to use the , illustrated in Figure 4. Figure 4: The Sidecar pattern. Rather than placing custom proxy logic in a shared layer, make it part of the new service. But rather than embedding the custom proxy in the service at compile-time, we use the and make the proxy a runtime binding activity. With this pattern, legacy clients use the protocol-translating proxy and new clients are offered the new service API. Inside the proxy, calls are translated and directed to the new service. That allows us to reuse the proxy if needed. More importantly, we can easily decommission the proxy when it is no longer needed by legacy clients, with minimal impact on the newer services. STEP 3: MIGRATE THE DATABASE Once we have identified the functional boundary and the interception method, we need to decide how we will approach database strangulation—that is, separating our legacy database from application services. We have a few paths to choose from. DATABASE FIRST In a database-first approach, we separate the schema first, which could potentially impact the legacy application. For example, a SELECT might require pulling data from two databases, and an UPDATE can lead to the need for distributed transactions. This option requires changes to the source application and doesn’t help us demonstrate progress in the short term. That is not what we are looking for. CODE FIRST A code-first approach lets us get to independently deployed services quickly and reuse the legacy database, but it could give us a false sense of progress. Separating the database can turn out to be challenging and hide future performance bottlenecks. But it is a move in the right direction and can help us discover the data ownership and what needs to be split into the database layer later. CODE AND DATABASE TOGETHER Working on the code and database together can be difficult to aim for from the get-go, but it is ultimately the end state we want to get to. Regardless of how we do it, we want to end up with a separate service and database; starting with that in mind will help us avoid refactoring later.   Figure 4.1: Database strangulation strategies Having a separate database requires data synchronization. Once again, we can choose from a few common technology approaches. TRIGGERS Most databases allow us to execute custom behavior when data is changed. In some cases, that could even be calling a web service and integrating with another system. But how triggers are implemented and what we can do with them varies between databases. Another significant drawback here is that using triggers requires changing the legacy database, which we might be reluctant to do. QUERIES We can use queries to regularly check the source database for changes. The changes are typically detected with implementation strategies such as timestamps, version numbers, or status column changes in the source database. Regardless of the implementation strategy, polling always leads to the dilemma between polling often and creating overhead over the source database, or missing frequent updates. While queries are simple to install and use, this approach has significant limitations. It is unsuitable for mission-critical applications with frequent database interactions. LOG READERS Log readers identify changes by scanning the database transaction log files. Log files exist for database backup and recovery purposes and provide a reliable way to capture all changes including DELETEs. Using log readers is the least disruptive option because they require no modification to the source database and they don’t have a query load. The main downside of this approach is that there is no common standard for the transaction log files and we'll need specialized tools to process them. This is where Debezium fits in.   Figure 4.2: Data synchronization patterns Before moving on to the next step, let's see how using Debezium with the log reader approach works. CHANGE DATA CAPTURE WITH DEBEZIUM When an application writes to the database, changes are recorded in log files, then the database tables are updated. For MySQL, the log file is binlog; for PostgreSQL, it is the write-ahead-log; and for MongoDB it's the op log. The good news is Debezium has connectors for different databases, so it does the hard work for us of understanding the format of all of these log files. Debezium can read the log files and produce a generic abstract event into a messaging system such as Apache Kafka, which contains the data changes. Figure 5 shows Debezium connectors as the interface for a variety of databases. Figure 5: Debezium connectors in a microservices architecture. Debezium is the most widely used open source change data capture (CDC) project with and features that make it a great fit for the Strangler pattern. WHY IS DEBEZIUM A GOOD FIT FOR THE STRANGLER PATTERN? One of the most important reasons to consider the Strangler pattern for migrating monolithic legacy applications is reduced risk and the ability to fall back to the legacy application. Similarly, Debezium is completely transparent to the legacy application, and it doesn’t require any changes to the legacy data model. Figure 6 shows Debezium in an example microservices architecture. Figure 6: Debezium deployment in a hybrid-cloud environment. With a minimal configuration to the legacy database, we can capture all the required data. So at any point, we can remove Debezium and fall back to the legacy application if we need to. DEBEZIUM FEATURES THAT SUPPORT LEGACY MIGRATIONS Here are some of Debezium's specific features that support migrating a monolithic legacy application with the Strangler pattern: * Snapshots: Debezium can take a snapshot of the current state of the source database, which we can use for bulk data imports. Once a snapshot is completed, Debezium will start streaming the changes to keep the target system in sync. * Filters: Debezium lets us pick which databases, tables, and columns to stream changes from. With the Strangler pattern, we are not moving the whole application. * Single message transformation (SMT): This feature can act like an anti-corruption layer and protect our new data model from legacy naming, data formats, and even let us filter out obsolete data * Using Debezium with a schema registry: We can use a schema registry such as with Debezium for schema validation, and also use it to enforce version compatibility checks when the source database model changes. This can prevent changes from the source database from impacting and breaking the new downstream message consumers. * Using Debezium with Apache Kafka: There are many reasons why Debezium and Apache Kafka work well together for application migration and modernization. Guaranteed ordering of database changes, message compaction, the ability to re-read changes as many times as needed, and tracking transaction log offsets are all good examples of why we might choose to use these tools together. STEP 4: RELEASING SERVICES With that quick overview of Debezium, let’s see where we are with the Strangler pattern. Assume that, so far, we have done the following: * Identified a functional boundary. * Migrated the functionality. * Migrated the database. * Deployed the service into a Kubernetes environment. * Migrated the data with Debezium and kept Debezium running to synchronize ongoing changes. At this point, there is not yet any traffic routed to the new services, but we are ready to release the new services. Depending on our routing layer's capabilities, we can use techniques such as dark launching, parallel runs, and canary releasing to reduce or remove the risk of rolling out the new service, as shown in Figure 7. Figure 7: Directing read traffic to the new service. What we can also do here is to only direct read requests to our new service initially, while continuing to send the writes to the legacy system. This is required as we are replicating changes in a single direction only. When we see that the read operations are going through without issues, we can then direct the write traffic to the new service. At this point, if we still need the legacy application to operate for whatever reason, we will need to stream changes from the new services toward the legacy application database. Next, we'll want to stop any write or mutating activity in the legacy module and stop the data replication from it. Figure 8 illustrates this part of the pattern implementation. Figure 8: Directing read and write traffic to the new service. Since we still have legacy read operations in place, we are continuing the replication from the new service to the legacy application. Eventually, we'll stop all operations in the legacy module and stop the data replication. At this point, we will be able to decommission the migrated module. We've had a broad look at using the Strangler pattern to migrate a monolithic legacy application, but we are not quite done with modernizing our new microservices-based architecture. Next, let’s consider some of the challenges that come later in the modernization process and how Debezium, Apache Kafka, and Kubernetes might help. AFTER THE MIGRATION: MODERNIZATION CHALLENGES The most important reason to consider using the Strangler pattern for migration is the reduced risk. This pattern gives value steadily and allows us to demonstrate progress through frequent releases. But migration alone, without enhancements or new “business value” can be a hard sell to some stakeholders. In the longer-term modernization process, we also want to enhance our existing services and add new ones. With modernization initiatives, very often, we are also tasked with setting the foundation and best practices for building modern applications that will follow. By migrating more and more services, adding new ones, and in general by transitioning to the microservices architecture, new challenges will come up, including the following: * Automating the deployment and operating a large number of services. * Performing dual-writes and orchestrating long-running business processes in a reliable and scalable manner. * Addressing the analytical and reporting needs. There are all challenges that might not have existed in the legacy world. Let’s explore how we can address a few of them using a combination of design patterns and technologies. CHALLENGE 1: OPERATING EVENT-DRIVEN SERVICES AT SCALE While peeling off more and more services from the legacy monolithic application, and also creating new services to satisfy emerging business requirements, the need for automated deployments, rollbacks, placements, configuration management, upgrades, self-healing becomes apparent. These are the exact features that make Kubernetes a great fit for operating large-scale microservices. Figure 9 illustrates. Figure 9: A sample event-driven architecture on top of Kubernetes. When we are working with event-driven services, we will quickly find that we need to automate and integrate with an event-driven infrastructure—which is where Apache Kafka and other projects in its ecosystem might come in. Moreover, we can use to help automate the management of Kafka and the following supporting services: * provides an Operator for managing Apicurio Schema Registry on Kubernetes. * offers Operators for managing Kafka and Kafka Connect clusters declaratively on Kubernetes. * (Kubernetes Event-Driven Autoscaling) offers workload auto-scalers for scaling up and down services that consume from Kafka. So, if the consumer lag passes a threshold, the Operator will start more consumers up to the number of partitions to catch up with message production. * offers event-driven abstractions backed by Apache Kafka. Note: Kubernetes not only provides a target platform for application modernization but also allows you to grow your applications on top of the same foundation into a large-scale event-driven architecture. It does that through automation of user workloads, Kafka workloads, and other tools from the Kafka ecosystem. That said, not everything has to run on your Kubernetes. For example, you can use a or a schema registry service from Red Hat and automatically bind it to your application using Kubernetes Operators. Creating a multi-availability-zone (multi-AZ) Kafka cluster on takes less than a minute and is completely free during our trial period. and help us shape it with your early feedback. Now, let’s see how we can meet the remaining two modernization challenges using design patterns. CHALLENGE 2: AVOIDING DUAL-WRITES Once you build a couple of microservices, you quickly realize that the hardest part about them is data. As part of their business logic, microservices often have to update their local data store. At the same time, they also need to notify other services about the changes that happened. This challenge is not so obvious in the world of monolithic applications and legacy distributed transactions. How can we avoid or resolve this situation the cloud-native way? The answer is to only modify one of the two resources—the database—and then drive the update of the second one, such as Apache Kafka, in an eventually consistent manner. Figure 10 illustrates this approach. Figure 10: The Outbox pattern. Using the with Debezium lets services execute these two tasks in a safe and consistent manner. Instead of directly sending a message to Kafka when updating the database, the service uses a single transaction to both perform the normal update and insert the message into a specific outbox table within its database. Once the transaction has been written to the database’s transaction log, Debezium can pick up the outbox message from there and send it to Apache Kafka. This approach gives us very nice properties. By synchronously writing to the database in a single transaction, the service benefits from "read your own writes" semantics, where a subsequent query to the service will return the newly persisted record. At the same time, we get reliable, asynchronous, propagation to other services via Apache Kafka. The Outbox pattern is a proven approach for avoiding dual-writes for scalable event-driven microservices. It solves the inter-service communication challenge very elegantly without requiring all participants to be available at the same time, including Kafka. I believe Outbox will become one of the foundational patterns for designing scalable event-driven microservices. CHALLENGE 3: LONG-RUNNING TRANSACTIONS While the Outbox pattern solves the simpler inter-service communication problem, it is not sufficient alone for solving the more complex long-running, distributed business transactions use case. The latter requires executing multiple operations across multiple microservices and applying consistent all-or-nothing semantics. A common example for demonstrating this requirement is the booking-a-trip use case consisting of multiple parts where the flight and accommodation must be booked together. In the legacy world, or with a monolithic architecture, you might not be aware of this problem as the coordination between the modules is done in a single process and a single transactional context. The distributed world requires a different approach, as illustrated in Figure 11.  Figure 11: The Saga pattern implemented with Debezium. The offers a solution to this problem by splitting up an overarching business transaction into a series of multiple local database transactions, which are executed by the participating services. Generally, there are two ways to implement distributed sagas: * Choreography: In this approach, one participating service sends a message to the next one after it has executed its local transaction. * Orchestration: In this approach, one central coordinating service coordinates and invokes the participating services. Communication between the participating services might be either synchronous, via HTTP or gRPC, or asynchronous, via messaging such as Apache Kafka. The cool thing here is that you can implement sagas using Debezium, Apache Kafka, and the Outbox pattern. With these tools, it is possible to take advantage of the orchestration approach and have one place to manage the flow of a saga and check the status of the overarching saga transaction. We can also combine orchestration with asynchronous communication to decouple the coordinating service from the availability of participating services and even from the availability of Kafka. That gives us the best of both worlds: orchestration and asynchronous, non-blocking, parallel communication with participating services, without temporal coupling. Combining the Outbox pattern with the Sagas pattern is an awesome, event-driven implementation option for the long-running business transactions use case in the distributed services world. See  (InfoQ) for a detailed description. Also see an of this pattern on GitHub. CONCLUSION The Strangler pattern, Outbox pattern, and Saga pattern can help you migrate from brown-field systems, but at the same time, they can help you build green-field, modern, event-driven services that are future-proof. Kubernetes, Apache Kafka, and Debezium are open source projects that have turned into de facto standards in their respective fields. You can use them to create standardized solutions with a rich ecosystem of supporting tools and best practices. The one takeaway from this article is the realization that modern software systems are like cities: They evolve over time, on top of legacy systems. Using proven patterns, standardized tools, and open ecosystems will help you create long-lasting systems that grow and change with your needs. This post was originally published on Red Hat Developers. To read the original post, check . </content><dc:creator>Unknown</dc:creator></entry><entry><title type="html">Getting started with Java 18</title><link rel="alternate" href="http://www.mastertheboss.com/java/upcoming-news-from-java-18/" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/java/upcoming-news-from-java-18/</id><updated>2022-03-26T11:09:00Z</updated><content type="html">Java 18 is finally available for download! In this article we will learn some of the most interesting Enhancement Proposals (JEPs) which are now available in the JDK. Installing Java 18 Firstly, let’s download the Java 18 platform from https://jdk.java.net/18/ Choose the version for your Operating System. Then, unpack it on your machine and set ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title type="html">How to inspect a Thread Dump like a pro</title><link rel="alternate" href="http://www.mastertheboss.com/java/how-to-inspect-a-thread-dump-like-a-pro/" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/java/how-to-inspect-a-thread-dump-like-a-pro/</id><updated>2022-03-25T15:41:09Z</updated><content type="html">This article will guide you in troubleshooting Java application by analysing a Thread dump with the instruments available in the JDK. We will also learn some tools to simplify our analysis. Java has mechanisms for analyzing the state of all threads of an application at a given time: Thread dumps. A thread dump is a ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry></feed>
